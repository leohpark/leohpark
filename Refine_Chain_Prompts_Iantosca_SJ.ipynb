{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCsdoqNpUdGun7ncKtjJU6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leohpark/leohpark/blob/main/Refine_Chain_Prompts_Iantosca_SJ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "This Notebook is an introduction for non-coders to explore the LangChain framework to create a Refine Chain Document Analyzer. Specifically here, I'm using it to create a structured data extractor for Copyright Claims.\n",
        "\n",
        "This extractor is a proof of concept demonstrating that extracting legal claims, entities, and outcomes is viable using LLMs like gpt-3.5-turbo. My expectation is that the solution posed here is relatively brittle, and I have not tested it with any rigor. In particular, I have no idea of the \"negation findings\" such as \"Copyright Not Infringed\" will work in cases where declaratory judgment is sought. If you switch out the source document with another Copyright SJ Order, I suspect the results will be more amusing than impressive.\n",
        "\n",
        "The technology stack used here is a Colab Notebook for a Python environment, then LangChain framework. \n",
        "\n",
        "###Disclaimer\n",
        "Unless required by applicable law or agreed to in writing, the code provided in this notebook is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "\n",
        "# Step 0 - Sign up for accounts with OpenAI. \n",
        "Link to sign up:\n",
        "\n",
        "- OpenAI: https://platform.openai.com/signup?launch\n",
        "\n",
        "Pages where your API keys are located\n",
        "\n",
        "- OpenAI: https://platform.openai.com/account/api-keys\n",
        "\n",
        "Request API keys and Store your API keys carefully. If someone has your keys, they can request services and incur costs from those services from these provides as if they are you. If your OpenAI key becomes misplaced or stolen, delete the key from your account and generate a new key.\n",
        "\n",
        "## Create some variables to store your API key for later\n",
        "This can be by entering the following commands:\n",
        "\n",
        "```\n",
        "OPEN_AI_KEY = \"...\"\n",
        "```\n",
        "where you copy/paste your keys into the \"...\" for each instruction.\n"
      ],
      "metadata": {
        "id": "oinPMZsjGGpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Input your OpenAI API Key here. We will need it later.\n",
        "OPEN_AI_KEY = \"...\""
      ],
      "metadata": {
        "id": "y7G8bCWzbh8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Install langchain framework and related packages\n",
        "Here we will be installing stuff to ensure that we have the tools necessary to execute the commands necessary to create our index and connect to Pinecone and OpenAI via API.\n",
        "Our Workflow Roadmap is:\n",
        "\n",
        "\t1. Get Frameworks and Packages installed.\n",
        "\t2. Find a copyright PDF to use as a data source. Use RecursiveCharacterTextSplitter to divide our source PDF into smaller chunks.\n",
        "\t3. Connect to our LLM\n",
        "\t4. Write a custom Refine Chain Prompt to extract data from this case, and run it using gpt-3.5-turbo.\n",
        "\n",
        "## Langchain framework\n",
        "Langchain is the framework full of useful tools and lots of fairly friendly defaults that will let us quickly write some code that will conect all of the pieces. The other two packages are required for processing PDF Documents."
      ],
      "metadata": {
        "id": "DxHqCaOHGDNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2 - Find a long PDF document as our Data Source. Use RecursiveCharacterTextSplitter to divide our source PDF into smaller chunks.\n",
        "The documetn we are analyzing is a Summary Judgment order from a social media copyright case, Iantosca v. Tahari: https://heitnerlegal.com/wp-content/uploads/copyright-infringement.pdf\n",
        "\n",
        "Chunking the PDF is necessary because most LLMS have relatively modest context limits, which is how much text they can consider in one instruction. For gpt-3.5.-turbo, the context limit is 4096 tokens, which corresponds to about 3000 words, or 9000 characters (including formatting). This document is 12 pages in length, which corresponds to about 6,000 words or 24,000 characters. Even though that is relatively short by legal standards, it is too long for gpt-3.5-turbo and many other available models to process at once while also considering a question, and formulating an answer.\n",
        "\n",
        "Chunk size corresponds roughly to the number of characters, although RecursiveCharacterTextSplitter will make some attempts to keep complete sentences or lines of text intact. It may be important/meaningful to calibrate your chunk size according to the type of information being ingested. \n",
        "\n",
        "##Refine Chain\n",
        "For this Refine Chain, I'm using 5,000 character chunks, which correspond to about 1000 tokens. This should leave plenty of Context for the gpt-3.5-turbo to consider the question and output.\n",
        "\n",
        "##Document Chunks\n",
        "We are using the `RecursiveCharacterTextSplitter`, which is included in the langchain framework. It goes through the document, attempting to separate it at the preferred separators, in separator odrrer, when it reaches the approxite chunk size. It will make variable chunk sizes based on the document formatting, and where it can find good natural divisions in the document.\n",
        "\n",
        "The separators here are \"double return\", \"single return\", \"space\" and then \"(no characters)\", which the splitter will only rely on when the document can't otherwise split the doc elsewhere. You might think \". \" would be a good separator for documents. For reasons I can't really explain, in my limited testing including that delimiter didn't appreciably improve the chunk separation I observed. The separator may already be factoring that in. \n",
        "\n",
        "One other thing, `chunk_overlap` only comes into play when a clean chunk division can't be found. So it makes sure that fragments of sentences appear on the next chunk when the previous chunk has a \"non-clean\" ending, e.g. cannot end with a sentence. It DOES NOT automatically buffer some part of a chunk into the next chunk for \"semantic overlap\" between chunks, as some tutorials suggest. You can easily confirm this by chunking documents and looking at the entire `texts` output.\n",
        "\n",
        "Try out the next line to see the contents of text blob #2"
      ],
      "metadata": {
        "id": "2gALzFUZNqDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install LangChain\n",
        "pip install langchain unstructured pdf2image"
      ],
      "metadata": {
        "id": "bnO-QVQOMJQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import PDF chunking tools from LangChain\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "rE68Y-FlNQPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload the PDF to Colab, then copy the pathname into the command below.\n",
        "loader = UnstructuredPDFLoader(\"/content/Iantosca v. Tahari.pdf\")\n"
      ],
      "metadata": {
        "id": "5W_j7Tw_--Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the file\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "UqVuvzWAQxGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title The following will tell us a bit about the PDF we just uploaded. We can check to see it loaded.\n",
        "print (f'You have {len(data)} document(s) in your data')\n",
        "print (f'There are {len(data[0].page_content)} characters in your document')"
      ],
      "metadata": {
        "id": "Zi0bXEEIQlnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this to see all of the data.\n",
        "data"
      ],
      "metadata": {
        "id": "2luYFnYpiadI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make Text Chunks\n",
        "# For Refine Chain queries, try 6000-8000 character chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(separators=['\\n\\n', '\\n', ' ', ''],  chunk_size=5000, chunk_overlap=200)\n",
        "\n",
        "texts = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "g37bfIXlRXi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check to se that your text chunks contain text\n",
        "texts[1]"
      ],
      "metadata": {
        "id": "_F1rUakmRhWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3 - Connect to our LLM\n",
        "\n",
        "We're going to install packages for Openai and tiktoken, which is sometimes necessary to manage our text chunks.\n",
        "\n",
        "After that, we'll import additional tools to run gpt-3.5-turbo, and to customize our Prompt Template. Customizing the prompt template is slightly advanced, but the default Refine Chain will not do what we are trying to accomplish here, which is structured text extraction."
      ],
      "metadata": {
        "id": "M_EGdUhaW2ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai tiktoken"
      ],
      "metadata": {
        "id": "un6YV2GSS6ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain import PromptTemplate\n"
      ],
      "metadata": {
        "id": "S9StrPdgZ9T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Write a custom refine chain and run it using gpt-3.5-turbo.\n",
        "\n",
        "A \"Refine Chain\" prompt consists of three prompts, a query, a question-prompt, and a refine-prompt. The query is the core prompt that is embedded in every step of the chain. The question-prompt is the first composed prompt consisting of the first text chunk, the query, and the question-template. This produces the first Answer.\n",
        "\n",
        "Then for each subsequent chunk, the refine-prompt combines the next text chunk, the query, the previous answer, and the refine-template to produce a new Answer. \n",
        "\n",
        "Here is the basic prompt chain logic:\n",
        "1. Define a json structure for data extraction as our \"query\".\n",
        "I'm using this structure:\n",
        "\n",
        "```\n",
        "query = \"\"\"\n",
        "“Case Number”: “”,\n",
        "“District Court”: “”,\n",
        "“Date”: “”,\n",
        "\"Plaintiffs\": [\"\"],\n",
        "\"Defendant\": [\"\"],\n",
        "“Claims”: [\n",
        "\t[“Legal Claim”: “”,\n",
        "\t“Movant”: “”,\n",
        "\t“Non-Movant”: “”,\n",
        "\t\"Claim Outcome: “”],\n",
        "\t]\n",
        "\"\"\"\n",
        "```\n",
        "2. Peform the question-prompt which contains additional definitions for the query, including a dictionary of claims, definitions for 'movant' and 'non-movant', and claim oucomes.\n",
        "\n",
        "3. Perform each refine-prompt using all of the same definitions, but each time instructing the LLM to answer using the same structure as the original query.\n",
        "\n",
        "I will try to do a separate write up of the thinking behind this prompt design soon. You can have the question-template and refine-template include different instructions, but that's not particularly helpful here. Using the 'query' prompt as an opportunity to enforce strict behavior over each iterative refine prompt greatly improves the uniformity of the structured text output. \n",
        "\n",
        "While this text extractor performs well on this SJ Order, they tend to be fairly brittle (meaning they do not perform well once the environment changes), and in general trying to get a consistent, repeatable, result from an LLM is fairly difficult. That's part of why I like building text extractors from gpt, I just enjoy impractical things. I'm also skeptical that certain concepts like the \"Copyright does not Infringe\" and other Declaratory Judgment type claims will be identified correctly. I hope this provides an intro to Refine Chain Prompting, and gives you some ideas as to how they can be used.\n"
      ],
      "metadata": {
        "id": "-t1Cu4kOaUxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Iantosca Prompt Templates for GPT3.5\n",
        "refine_template = (\n",
        "    \"Context:{context_str}\"\n",
        "    \"Sample Answer {question}\\n\"\n",
        "    \"Revised Answer: {existing_answer}\\n\"\n",
        "    \"Use the Context to collect as many Legal Claims. Claims include ('Copyright Valid', 'Copyright Invalid', 'Copyright Infringed', 'Copyright does not Infringe', 'Fair Use Defense', 'Fair Use Defense Not Available').\"\n",
        "    \"Movant is the parties name bringing the claim. Non-Movant is the parties name arguing against the claim.\"\n",
        "    \"Claim Outcome is Granted, Granted-in-Part, or Not Granted\"\n",
        "    \"New Revised Answer that matches the structure of the Sample Answer.\"\n",
        ")\n",
        "refine_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
        "    template=refine_template,\n",
        ")\n",
        "\n",
        "\n",
        "question_template = (\n",
        "    \"Context: {context_str}\"\n",
        "    \"Use the Context to collect as many Legal Claims. Claims include ('Copyright Valid', 'Copyright Invalid', 'Copyright Infringed', 'Copyright does not Infringe', 'Fair Use Defense', 'Fair Use Defense Not Available').\"\n",
        "    \"Movant is the parties name bringing the claim. Non-Movant is the parties name arguing against the claim.\"\n",
        "    \"Claim Outcome is Granted, Granted-in-Part, or Not Granted\"\n",
        "    \"{question}\\n\"\n",
        ")\n",
        "question_prompt = PromptTemplate(\n",
        "    input_variables=[\"context_str\", \"question\"], template=question_template\n",
        ")"
      ],
      "metadata": {
        "id": "rBxki530ISui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure our LLM and Chain prompt for Iantosca SJ\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_KEY, model_name=\"gpt-3.5-turbo\", max_tokens=1400)\n",
        "chain = load_qa_with_sources_chain(llm, chain_type=\"refine\", verbose=True, return_intermediate_steps=True, question_prompt=question_prompt, refine_prompt=refine_prompt)\n"
      ],
      "metadata": {
        "id": "MOgl87yQLoxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Iantosca Run Chain\n",
        "query = \"\"\"“Case Number”: “”,\n",
        "“District Court”: “”,\n",
        "“Date”: “”,\n",
        "\"Plaintiffs\": [\"\"],\n",
        "\"Defendant\": [\"\"],\n",
        "“Claims”: [\n",
        "\t[“Legal Claim”: “”,\n",
        "\t“Movant”: “”,\n",
        "\t“Non-Movant”: “”,\n",
        "\t\"Claim Outcome: “”],\n",
        "\t]\n",
        "\"\"\"\n",
        "chain({\"input_documents\": texts, \"question\": query})"
      ],
      "metadata": {
        "id": "7AmD7sRyISuj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}