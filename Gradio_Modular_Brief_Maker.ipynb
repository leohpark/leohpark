{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/BqZEgw054Y6awbSJiLWu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leohpark/leohpark/blob/main/Gradio_Modular_Brief_Maker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Modular Brief Maker\n",
        "\n",
        "This is a proof of concept colab notebook to explore modular prompt templating to generate legal case briefs. The brief-generating prompt can be configured to contain different brief sections, as well as include \"focus topics\" from inferred keywords.\n",
        "\n",
        "Due to the complexity limitations of gpt-3.5 and the loss of performance over long contexts, this is more conceptual than useful at this point. However, I think it's an interesting technique for either rapidly prototyping different prompting techniques, or developing summarization techniques that incorporate some self-prioritization or human-assigned subejct matter prioritization.\n",
        "\n",
        "## Summaries\n",
        "\n",
        "Click \"Upload\" to upload a pdf or simply copy paste text into the \"Summary Outline\" textbox. The subsequent prompts are designed to use an outline, but will work with any text source pasted into the window. I'm not entirely sure what the end product will look like though.\n",
        "\n",
        "\"Doc Tokens\" will tell you how many tokens are in the Doc you uploaded, so that can understand how much compression occurs from the source material and the Outline.\n",
        "\n",
        "Subsequent model calls, \"Generate Legal Topics\" and \"Get Final Brief!\", both dynamically pull text from the \"Summary Outline\" textbox, so changes can be made to this box if you want to test different text combinations. Be aware that if you exceed ~12500 tokens, those API calls may fail due to exceeding gpt-3.5-turbo-16k's context length.\n",
        "\n",
        "Configure the \"Doc Chunk Size\" and \"Output Brief Size\" (recommended values of 2000, 2000 work well). Then click \"Get Summaries\" to generate a Summary Outline of approximately 9000-12500 tokens in length using gpt-3.5-turbo.\n",
        "\n",
        "\"Outline Tokens\" will dynamically keep track of however much text you have in the Summary Outline.\n",
        "\n",
        "## Make your Brief!\n",
        "\"Legal Brief Maker\" is where the Final Brief text will eventually be placed.\n",
        "\n",
        "\"Prompt Preview\" will dynamically display the modular components that will be fed into the prompt template for the Final Brief.\n",
        "\n",
        "\"Prompt Builder\" allows you to add and subtract brief sections from your final brief. It's a bunch of checkboxes.\n",
        "\n",
        "\"Generated Topics\" is a multi-select dropdown. You need to generate legal topics before this menu is populated. If you spam-click this too aggressively, gradio can encounter bugs.\n",
        "\n",
        "Click \"Generate Legal Topics\" to make an API call that will generate inferred keywords from the text in \"Summary Outline.\" In an application setting this would normally be handled automatically, but since I don't have output validation from the gpt call it sometimes breaks.\n",
        "\n",
        "\"Generate Final Brief!\" will take the Prompt Builder and Generated Topics settings and place them into a prompt template used to create the final brief, output to \"Legal Brief Maker.\" You can then mess around with settings and see how it affects your brief.\n",
        "\n",
        "##Debugging\n",
        "\n",
        "Mostly for self-checking things while I'm building."
      ],
      "metadata": {
        "id": "sMbnH-9buTeH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwU2S8vH4Pvv"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio langchain unstructured pdf2image openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu2xE86D3nMH"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.docstore import document\n",
        "import tiktoken\n",
        "import openai\n",
        "import json\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "def tiktoken_len(text):\n",
        "  tokens = tokenizer.encode(\n",
        "      text,\n",
        "      disallowed_special=()\n",
        "  )\n",
        "  return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing this double key binding thing because some api calls are via langchain, and others are directly.\n",
        "OPENAI_KEY = \"...\"\n",
        "openai.api_key = OPENAI_KEY"
      ],
      "metadata": {
        "id": "L5eq1WPWuY3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MviO6eTHE2j"
      },
      "outputs": [],
      "source": [
        "#@title Customizable Brief Maker 1.0\n",
        "\n",
        "summary_template_default = \"\"\"Context:{text}\n",
        "\n",
        "As an experienced legal analyst, review the Context which is part of a long document to answer create a detailed and comprehensive\n",
        "bulleted outline of the Context with as many facts, descriptions, explanations, reasonings, previous case citations and their relationship to this case, as you can.\n",
        "\"\"\"\n",
        "\n",
        "formatted_brief_default = \"\"\"Case: [name of the case, court, year]\n",
        "Questions Presented: The issues the court case must resolve.\n",
        "Facts of the Case: the parties, facts and events leading to this court case.\n",
        "Procedural History: [district court case summary, appeals court case summary, how this issue reached this court]\n",
        "Rules: detailed explanation of relevant statutes, interpretations, standards, and tests applicable to this case\n",
        "Case Law: names of cases and analysis of how they relate to the Questions Presented\n",
        "Application: detailed analysis of how the Rules and Case Law help reach the conclusions\n",
        "Conclusion: the court's answer to the Questions Presented.\"\"\"\n",
        "\n",
        "inferred_topics_template_default = \"\"\"Context:{text}\n",
        "You are an experienced attorney and legal scholar reviewing legal documents. Return a JSON of the ten most repeated legal key phrases from the Context,\n",
        "Topics=[\"topic 1\", \"topic 2\"...]. Return only the JSON.\"\"\"\n",
        "\n",
        "final_template_default = \"\"\"Context:{text}\n",
        "As an experienced legal analyst, use the Context to compose a case memorandum working step-by-step using only information from the Context.\n",
        "Replace the bullet points with paragraphs in the following memo structure:\n",
        "\n",
        "{formatted_brief}\n",
        "\"\"\"\n",
        "\n",
        "topics_choices = []\n",
        "\n",
        "#function that selects PDF, extracts text to list, extracts page_content, counts tokens, sets token ratio.\n",
        "\n",
        "def upload_file(files, token_ratio):\n",
        "  page_content = []\n",
        "  loader = UnstructuredPDFLoader(files.name)\n",
        "  docs_raw = loader.load()\n",
        "  doc_content = docs_raw[0].page_content[:]\n",
        "  docs_tokens = int(tiktoken_len(doc_content))\n",
        "\n",
        "  # This line here lets you tune the max token ratio!!!\n",
        "  # if you are using GPT4, then the maximum tokens for summaries is probably around 5500-6000.\n",
        "  new_token_ratio = round(docs_tokens / 12500, 1)\n",
        "  if new_token_ratio > token_ratio:\n",
        "    token_ratio = new_token_ratio\n",
        "  else:\n",
        "    token_ratio = 2.5\n",
        "  return doc_content, docs_tokens, token_ratio\n",
        "\n",
        "def get_tokens(summary_outline):\n",
        "  summary_docs_tokens = int(tiktoken_len(summary_outline))\n",
        "  return summary_docs_tokens\n",
        "\n",
        "def summarize_it(doc, chunk_s, summary_template, token_ratio):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = chunk_s, #chunk_s, # number of units per chunk\n",
        "      chunk_overlap = 0, # number of units of overlap\n",
        "      length_function = tiktoken_len, #trying to use tokens as chunking unit instead of characters.\n",
        "      separators=['\\n\\n', '\\n', ' ', ''] # our chosen operators for separating\n",
        "      )\n",
        "  texts = text_splitter.split_text(doc)\n",
        "\n",
        "  s_prompt = PromptTemplate(\n",
        "      input_variables=[\"text\"],\n",
        "      template=summary_template\n",
        "  )\n",
        "\n",
        "  # Use this for OpenAI latest: gpt-3.5-turbo, bigger context: gpt-3.5-turbo-16k, functions: gpt-3.5-turbo-0613\n",
        "  llms = OpenAI(temperature=0, openai_api_key=OPENAI_KEY, model_name=\"gpt-3.5-turbo\", max_tokens=int(chunk_s // token_ratio))\n",
        "  summarized_texts = \"\"\n",
        "  for text in texts:\n",
        "    summary_prompt = s_prompt.format(text=text)\n",
        "    summary = llms(summary_prompt)\n",
        "    summarized_texts += summary + \"\\n\"\n",
        "  summarized_tokens = int(tiktoken_len(summarized_texts))\n",
        "\n",
        "  return summarized_texts, summarized_tokens\n",
        "\n",
        "# function call to rerun the Final Summary\n",
        "\n",
        "def final_brief(summarized_texts, formatted_brief, out_s):\n",
        "  formatted_brief = formatted_brief\n",
        "\n",
        "  f_prompt = PromptTemplate(\n",
        "      input_variables=[\"text\", \"formatted_brief\"],\n",
        "      template=final_template_default\n",
        "  )\n",
        "  llmf = OpenAI(temperature=0, openai_api_key=OPENAI_KEY, model_name=\"gpt-3.5-turbo-16k\", max_tokens=out_s)\n",
        "  final_prompt = f_prompt.format(text=summarized_texts, formatted_brief=formatted_brief)\n",
        "  final_summary = llmf(final_prompt)\n",
        "  final_doc_tokens = int(tiktoken_len(final_summary))\n",
        "  return final_summary, final_template_default, final_doc_tokens\n",
        "\n",
        "#this function takes the summary_outline and does a direct API call for a list of topics.\n",
        "#I was going to use a function call, but they can't return an array as far as I can see.\n",
        "\n",
        "def get_topics(summary_outline):\n",
        "  topics_debug = []\n",
        "  topics_messages = [{\"role\": \"system\", \"content\":\n",
        "                      \"\"\"You are an experienced attorney and legal scholar reviewing legal documents.\n",
        "                      Return a JSON array \\\"legal_topics\\\" of the twelve most repeated legal key phrases from the\n",
        "                      Context. Answer: \"legal_topics\":[\"topic 1\", \"topic 2\", \"topic 3\", \"topic 4\"...\"topic 12\"] \"\"\"},\n",
        "                      {\"role\": \"user\", \"content\": summary_outline},]\n",
        "\n",
        "  #gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-4-0613, gpt-4-32k-0613\n",
        "  topics_list = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo-16k-0613\",\n",
        "      temperature=0,\n",
        "      messages=topics_messages\n",
        "      )\n",
        "  response_message=topics_list[\"choices\"][0][\"message\"]\n",
        "  response_json = json.loads(response_message['content'])\n",
        "  legal_topics = response_json['legal_topics']\n",
        "  topics_choices = [topic.strip() for topic in legal_topics]\n",
        "  topics_choices = [str(topic).strip() for topic in legal_topics]\n",
        "\n",
        "  return gr.Dropdown.update(choices=topics_choices), topics_choices\n",
        "\n",
        "#This function assembles the final prompt to be performed by Brief Builder.\n",
        "def prompt_preview(brief_selection, topics_list):\n",
        "  case_name = \"Case: [name of the case, court, year]\\n\"\n",
        "  questions_presented = \"Questions Presented: The issues the court case must resolve.\\n\"\n",
        "  brief_bits = \"\"\"\"\"\"\n",
        "  if 'Facts' in brief_selection:\n",
        "    brief_bits += \"Facts of the Case: the parties, facts and events leading to this court case.\\n\"\n",
        "  if 'Procedural' in brief_selection:\n",
        "    brief_bits += \"Procedural History: [district court case summary, appeals court case summary, how this issue reached this court]\\n\"\n",
        "  if 'Rules' in brief_selection:\n",
        "    brief_bits += \"Rules: detailed explanation of relevant statutes, interpretations, standards, and tests applicable to this case\\n\"\n",
        "  if 'Case Law' in brief_selection:\n",
        "    brief_bits += \"Case Law: names of cases and analysis of how they relate to the Questions Presented\\n\"\n",
        "  if 'Application' in brief_selection:\n",
        "    brief_bits += \"Application: detailed analysis of how the Rules and Case Law help reach the conclusions\\n\"\n",
        "  if 'Conclusion' in brief_selection:\n",
        "    brief_bits += \"Conclusion: the court's answer to the Questions Presented.\\n\"\n",
        "  if 'Dissent' in brief_selection:\n",
        "    brief_bits += \"Dissent: (exclude if a dissent was not included) How it disagrees with the holding of this case.\\n\"\n",
        "\n",
        "  foci_bits = \"\"\n",
        "  if topics_list:\n",
        "   # foci_bits = \"Review the Context carefully to provide as much detail as you can in the memo about \" + \", \".join([\"'{}'\".format(topic) for topic in topics_list])\n",
        "   json_string = json.dumps({\"Memo Focus\": topics_list})\n",
        "   foci_bits = appended_json_string = json_string + \" - Memo Focus: Review the Context step by step for these topics and discuss them thoroughly in the memo.\"\n",
        "\n",
        "  else:\n",
        "    foci_bits = \"\"\n",
        "\n",
        "  formatted_brief = case_name + questions_presented + brief_bits + \"\\n\" + foci_bits\n",
        "\n",
        "  return formatted_brief, formatted_brief, brief_selection, topics_list\n",
        "\n",
        "#Gradio (Gradio)! All we hear is Gradio Gaga! Gradio Goo Goo!\n",
        "#themes to try: \"default\", \"huggingface\", \"grass\", \"peach\", \"darkdefault\", \"darkhuggingface\", \"darkgrass\", \"darkpeach\"\n",
        "# gradio themes from gradio gr.themes.Base() gr.themes.Default(), gr.themes.Glass(), gr.themes.Monochrome(), gr.themes.Soft()\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as briefinator:\n",
        "  with gr.Row():\n",
        "    with gr.Column(scale=2.5):\n",
        "      with gr.Tab(\"Summaries\"):\n",
        "        summary_doc = gr.Textbox(lines=12, max_lines=15, label=\"Summary Outline\", show_copy_button=True)\n",
        "        summary_save = gr.State(summary_template_default)\n",
        "\n",
        "        with gr.Row():\n",
        "          doc_tokens = gr.Textbox(label=\"Doc Tokens\", scale=1)\n",
        "          chunk_s = gr.Slider(1000, 2500, step=100, label=\"Doc Chunk Size\", scale=4, value=2000)\n",
        "          out_s = gr.Slider(1000, 4000, step=250, label=\"Output Brief Size\", scale=4, value=2000)\n",
        "          outline_tokens = gr.Textbox(label=\"Outline Tokens\", scale=1)\n",
        "          summary_doc.change(fn=get_tokens, inputs=summary_doc, outputs=outline_tokens)\n",
        "\n",
        "# Document upload thingy\n",
        "\n",
        "        with gr.Row():\n",
        "          uploaded_doc = gr.State([])\n",
        "          token_ratio = gr.State(2.5)\n",
        "          upl_btn = gr.UploadButton(\"Upload PDF\", file_types=[\".pdf\"], file_count=\"single\", size=\"sm\")\n",
        "          upl_btn.upload(fn=upload_file, inputs=[upl_btn, token_ratio], outputs=[uploaded_doc, doc_tokens, token_ratio])\n",
        "          summarize_button = gr.Button(\"Get Summaries\", variant=\"primary\", size=\"sm\")\n",
        "\n",
        "# Legal Brief Maker tab\n",
        "      with gr.Tab(\"Make your Brief!\"):\n",
        "        brief_box = gr.Textbox(lines=12,interactive=True, label=\"Legal Brief Maker\", show_copy_button=True)\n",
        "        with gr.Row():\n",
        "          with gr.Column():\n",
        "            brief_settings_state = gr.State([\"Facts\", \"Procedural\",\"Rules\", \"Case Law\", \"Application\", \"Conclusion\"])\n",
        "            pick_topics_state = gr.State([])\n",
        "            formatted_brief_state = gr.State(formatted_brief_default)\n",
        "            #brief_preview is not visible here for UI clarity, but the information is still visible from the debugger.\n",
        "            brief_preview = gr.Textbox(lines = 5, label =\"Prompt Preview\", info=\"Brief Bits Made to Order\")\n",
        "            brief_settings = gr.CheckboxGroup(choices=[\"Facts\", \"Procedural\",\"Rules\", \"Case Law\", \"Application\", \"Conclusion\", \"Dissent\"],\n",
        "                                              value=[\"Facts\", \"Procedural\",\"Rules\", \"Case Law\", \"Application\", \"Conclusion\"],\n",
        "                                              label=\"Prompt Builder\", info=\"Select which Sections you want in the Final Brief.\", interactive=True)\n",
        "\n",
        "            pick_topics = gr.Dropdown(choices=topics_choices, multiselect=True, max_choices=6, label=\"Generated Topics\", interactive=True,\n",
        "                                      info = \"Inferred Legal Topics, curated by GPT! Multiselect up to six topics to focus the brief\")\n",
        "            brief_settings.change(fn=prompt_preview, inputs=[brief_settings, pick_topics_state], outputs=[ brief_preview, formatted_brief_state, brief_settings_state, pick_topics_state])\n",
        "            pick_topics.change(fn=prompt_preview, inputs=[brief_settings_state, pick_topics], outputs=[ brief_preview, formatted_brief_state, brief_settings_state, pick_topics_state])\n",
        "\n",
        "        #translates outline into legal topics for Brief Foci\n",
        "        with gr.Row():\n",
        "          topics_button = gr.Button(\"Generate Legal Topics\", variant=\"primary\", size=\"sm\")\n",
        "          final_button = gr.Button(\"Get Final Brief!\", variant=\"stop\", size=\"sm\")\n",
        "\n",
        "      #some outputs to keep a check on things\n",
        "      with gr.Tab(\"Debugging\"):\n",
        "        with gr.Row():\n",
        "          summaries_tokens = gr.Textbox(scale=1, label=\"Final Brief Tokens\")\n",
        "        final_prompt = gr.Textbox(lines=15, label=\"Final Prompt for Viewing\", show_copy_button=True)\n",
        "        topics_return = gr.Textbox(lines=15, label=\"What was returned from 0613 for topics JSON\", show_copy_button=True)\n",
        "\n",
        "\n",
        "# Button values\n",
        "\n",
        "        topics_button.click(fn=get_topics, inputs=[summary_doc], outputs=[pick_topics, topics_return])\n",
        "        summarize_button.click(fn=summarize_it, inputs=[uploaded_doc, chunk_s, summary_save, token_ratio], outputs=[summary_doc, outline_tokens])\n",
        "        final_button.click(fn=final_brief, inputs=[summary_doc, formatted_brief_state, out_s], outputs =[brief_box, final_prompt, summaries_tokens])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    briefinator.queue().launch(share=True, debug=True)"
      ]
    }
  ]
}